{"cells":[{"metadata":{"_uuid":"ad61629c8ff5de83aa7d127175a7e46bfc394a21","_cell_guid":"a33bd49a-49b1-4d8e-ba8d-ff0c902e1dca"},"cell_type":"markdown","source":"# Wisconsin Breast Cancer Machine Learning\n\n+ Contributor: Raul Eulogio, Edits by Brittany Marie Swanson\n+ Originally posted on [inertia7](https://www.inertia7.com/projects/95)\n+ Also contributed to [Data Science Inc.](https://www.datascience.com/resources/notebooks/random-forest-intro)\n\n# Abstract \n\nFor this project, I implemented a **Random Forest Model** on a data set containing descriptive attributes of digitized images of a process known as, *fine needle aspirate* (**FNA**) of breast mass. \n\nWe have a total of 29 features that were computed for each cell nucleus with an ID Number and the Diagnosis (Later converted to binary representations: *Malignant* = **1**, *Benign* = **0**). \n\n<img src=\"https://www.researchgate.net/profile/Glaucia_Sizilio/publication/232811011/figure/fig3/AS:214220353347586@1428085520095/Captured-images-of-layers-of-glass-with-smears-of-breast-massobtained-by-FNA-the-parts.png\">\n\n\n+ Ex. Image of a malignant solitary fibrous tumor using **FNA**. \n+ Source by [Glaucia Rma Sizilio](https://www.researchgate.net/profile/Glaucia_Sizilio), [Cicilia Rm Leite](https://www.researchgate.net/profile/Cicilia_Leite), [Ana Mg Guerreiro](https://www.researchgate.net/scientific-contributions/2069488816_Ana_Mg_Guerreiro), and [Adriao D Doria Neto](https://www.researchgate.net/profile/Adriao_Duarte_Neto)\n\n## Origins\n\nThis data set originated in early 1990's, when Dr. William H. Wolberg was curious if he could find a way to accurately predict breast cancer diagnosis based on **FNA**'s. \n\nThe research was broken down into two parts; the extraction of the data (which we will go over) and the classficattion (if you want to read more find information on this section [here](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.74.6745&rep=rep1&type=pdf)). \n\n### Steps Taken\n\n+ **FNA**'s were done on a total of 569 patients, once done the samples were then stained to help differentiate distinguished cell nuclei\n+ Samples were classified as cancer-based through biopsy and historical confirmation. Non-cancer samples were confirmed by biopsy or follow ups. \n+ Users then chose areas of the **FNA** with minimal overlap between nuclei; they then took scans utilizing a digital camera. \n+ Using a software called `Xcyt`, the team created approximate boundaries, which would then used a process called [snakes](https://en.wikipedia.org/wiki/Active_contour_model) which converged to give the exact nuclei boundary. \n+ Finally, once the boundaries for the nuclei were set, calculations were made resulting in 29 features, creating this data set!\n\nMore information regarding the process can be found [here](http://pages.cs.wisc.edu/~olvi/uwmp/cancer.html). I will source them at the end of the project as well, but I found these to be interesting reads especially since I've seen the data set used heavily without a lot of context as to how the data was actually extracted. More information [here](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.74.6745&rep=rep1&type=pdf)\n\n## Introduction to Random Forest\n\nRandom forests, also known as random decision forests, are a popular ensemble method that can be used to build predictive models for both classification and regression problems. Ensemble methods use multiple learning models to gain better predictive results — in the case of a random forest, the model creates an entire forest of random uncorrelated decision trees to arrive at the best possible answer.\n\nTo demonstrate how this works in practice — specifically in a classification context — I’ll be walking you through an example using a famous data set from the University of California, Irvine (UCI) Machine Learning Repository. The data set, called the Breast Cancer Wisconsin (Diagnostic) Data Set, deals with binary classification and includes features computed from digitized images of biopsies. The data set can be downloaded [here](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29).\n\nTo follow this tutorial, you will need some familiarity with classification and regression tree (CART) modeling. I will provide a brief overview of different CART methodologies that are relevant to random forest, beginning with decision trees. If you’d like to brush up on your knowledge of CART modeling before beginning the tutorial, I highly recommend reading Chapter 8 of the book “An Introduction to Statistical Learning with Applications in R,” which can be downloaded [here](http://www-bcf.usc.edu/~gareth/ISL/).\n\n## Decision Trees\n\nDecision trees are simple but intuitive models that utilize a top-down approach in which the root node creates binary splits until a certain criteria is met. This binary splitting of nodes provides a predicted value based on the interior nodes leading to the terminal (final) nodes. In a classification context, a decision tree will output a predicted target class for each terminal node produced.\n\nAlthough intuitive, decision trees have limitations that prevent them from being useful in machine learning applications. You can learn more about implementing a decision tree [here](http://scikit-learn.org/stable/modules/tree.html).\n\n### Limitations to Decision Trees\n\nDecision trees tend to have high variance when they utilize different training and test sets of the same data, since they tend to overfit on training data. This leads to poor performance on unseen data. Unfortunately, this limits the usage of decision trees in predictive modeling. However, using ensemble methods, we can create models that utilize underlying decision trees as a foundation for producing powerful results.\n\n## Bootstrap Aggregating Trees\n\nThrough a process known as bootstrap aggregating (or bagging), it’s possible to create an ensemble (forest) of trees where multiple training sets are generated with replacement, meaning data instances — or in the case of this tutorial, patients — can be repeated. Once the training sets are created, a CART model can be trained on each subsample.\n\nThis approach helps reduce variance by averaging the ensemble's results, creating a majority-votes model. Another important feature of bagging trees is that the resulting model uses the entire feature space when considering node splits. Bagging trees allow the trees to grow without pruning, reducing the tree-depth sizes and resulting in high variance but lower bias, which can help improve predictive power.\n\nHowever, a downside to this process is that the utilization of the entire feature space creates a risk of correlation between trees, increasing bias in the model.\n\n### Limitations to Bagging Trees\n\nThe main limitation of bagging trees is that it uses the entire feature space when creating splits in the trees. If some variables within the feature space are indicative of certain predictions, you run the risk of having a forest of correlated trees, thereby increasing bias and reducing variance.\n\nHowever, a simple tweak of the bagging trees methodology can prove advantageous to the model’s predictive power.\n\n## Random Forest\n\nRandom forest aims to reduce the previously mentioned correlation issue by choosing only a subsample of the feature space at each split. Essentially, it aims to make the trees de-correlated and prune the trees by setting a stopping criteria for node splits, which I will cover in more detail later."},{"metadata":{"_uuid":"fd0ebe724ba6eccfe98db262b473ebd63b4692d8","collapsed":true,"_cell_guid":"01709b27-2ad5-4ba4-90f9-3c5df4401602","trusted":true},"cell_type":"code","source":"# Import modules\n%matplotlib inline\n\nimport time\nimport random\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier \nfrom urllib.request import urlopen \n\nplt.style.use('ggplot')\npd.set_option('display.max_columns', 500) ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"69bdcc3fbaec94e6fd46e4c21bc12db1028ebb21","_cell_guid":"5df70f4a-52e0-4654-9fbc-7b012ce987d9"},"cell_type":"markdown","source":"# Load Data\nFor this section, I'll load the data into a `Pandas` dataframe"},{"metadata":{"_uuid":"874eed3237b8fc1c4e03ad44fb00904623afc997","collapsed":true,"_cell_guid":"aee303ba-c5af-4b46-b6b4-375b3aa8ca9e","trusted":true},"cell_type":"code","source":"breast_cancer = pd.read_csv('../input/data.csv')\nnames = ['id', 'diagnosis', 'radius_mean', \n         'texture_mean', 'perimeter_mean', 'area_mean', \n         'smoothness_mean', 'compactness_mean', \n         'concavity_mean','concave_points_mean', \n         'symmetry_mean', 'fractal_dimension_mean',\n         'radius_se', 'texture_se', 'perimeter_se', \n         'area_se', 'smoothness_se', 'compactness_se', \n         'concavity_se', 'concave_points_se', \n         'symmetry_se', 'fractal_dimension_se', \n         'radius_worst', 'texture_worst', \n         'perimeter_worst', 'area_worst', \n         'smoothness_worst', 'compactness_worst', \n         'concavity_worst', 'concave_points_worst', \n         'symmetry_worst', 'fractal_dimension_worst'] \n\ndx = ['Benign', 'Malignant']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d2d08e9986d002a2e297a82ed2391ea102889fa0","_cell_guid":"2d78c464-6b16-4ba5-9463-c94cd192f004"},"cell_type":"markdown","source":"## Cleaning\nWe do some minor cleanage like setting the `id_number` to be the data frame index, along with converting the `diagnosis` to the standard binary 1, 0 representation using the `map()` function. "},{"metadata":{"_uuid":"f28372c42a1973e615141052d138935b2bd0c881","collapsed":true,"_cell_guid":"938abf46-fb33-4893-9006-17010ae73795","trusted":true},"cell_type":"code","source":"# Setting 'id_number' as our index\nbreast_cancer.set_index(['id'], inplace = True) \n# Converted to binary to help later on with models and plots\nbreast_cancer['diagnosis'] = breast_cancer['diagnosis'].map({'M':1, 'B':0})","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0375b0ba490db13c2c834eb85cbccb237f770448","_cell_guid":"7933149f-d963-4484-9235-b75d0858bdfd"},"cell_type":"markdown","source":"## Missing Values\nGiven context of the data set, I know that there is no missing data, but I ran an `apply` method utilizing a lambda expression that checks to see if there was any missing values through each column. Printing the column name and total missing values for that column, iteratively. "},{"metadata":{"_uuid":"85f27da7ab53882fc1d9d42a21069461868a2751","_cell_guid":"cfbafedd-7602-4c3a-90d4-07d3b878292b","trusted":true},"cell_type":"code","source":"breast_cancer.apply(lambda x: x.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0d7751561e2d3ec29ae64925e05c53afb695277e","_cell_guid":"79b6dba3-79f2-4cea-aa0b-695926a4ae4a"},"cell_type":"markdown","source":"We have to delete this extra column since it doesn't contain any data. \n\nThis following code  will be used for the random forest model, where the `id_number` won't be relevant. "},{"metadata":{"_uuid":"088142942679342236d93f2b1245ad0651ce5f06","collapsed":true,"_cell_guid":"cfd35d61-dc44-4cab-80bf-9f271ddfc6f8","trusted":true},"cell_type":"code","source":"# For later use in CART models\nnames_index = names[2:]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"01a85bf4f5dd729ac922d16b22c210f48b6ab4a9","_cell_guid":"c7011056-4503-4e1d-88f2-4886fd946a52"},"cell_type":"markdown","source":"Here we're deleting the extra column"},{"metadata":{"_uuid":"05a6a5cb016a0174bf848f4150e90cd93c7758c9","_cell_guid":"3868dc78-cbfb-491d-be9f-f9a74871aaaa","trusted":true,"collapsed":true},"cell_type":"code","source":"del breast_cancer['Unnamed: 32']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f1208f7cb380d78e01ff8783c8d332a82336dcfc"},"cell_type":"markdown","source":"Let's preview the data set utilizing the `head()` function which will give the first 5 values of our data frame. "},{"metadata":{"_uuid":"f6d3f04c36334fa56123e9a57d0d3190592b3361","_cell_guid":"36144d75-4a48-4b45-9c1d-5d010cd30ed5","trusted":true},"cell_type":"code","source":"breast_cancer.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7cf8107de1283dda8468dde89d069a336b6e8744","_cell_guid":"6fed6f8c-6c1b-4856-a58a-bfebfbae2a65"},"cell_type":"markdown","source":"Next, we'll give the dimensions of the data set; where the first value is the number of patients and the second value is the number of features. \n\nWe print the data types of our data set this is important because this will often be an indicator of missing data, as well as giving us context to anymore data cleanage. "},{"metadata":{"_uuid":"9d4994fde778ffa9e010178e0e997c897289fcb3","_cell_guid":"cb634838-ec35-493e-ba1f-993c93fb0a0a","trusted":true},"cell_type":"code","source":"print(\"Here's the dimensions of our data frame:\\n\", \n     breast_cancer.shape)\nprint(\"Here's the data types of our columns:\\n\",\n     breast_cancer.dtypes)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a50ac9adbd9f45a5fb95c2b5276dcd771f1bf96a","_cell_guid":"58b1b4c9-1660-45d7-8e9b-c858f2a7e287"},"cell_type":"markdown","source":"## Class Imbalance\nThe distribution of diagnoses is important because it speaks to class imbalance within machine learning and data mining applications. Class imbalance is a term used to describe when a target class within a data set is outnumbered by another target class (or classes). This can create misleading accuracy metrics, known as an accuracy paradox. To make sure our target classes aren't imbalanced, create a function that will output the distribution of the target classes.\n\n**Note**: If your data set suffers from class imbalance, I suggest reading up on upsampling and downsampling."},{"metadata":{"_uuid":"c575ade0bc733e4f3c1aa1594cc821ab91f6c1be","collapsed":true,"_cell_guid":"625f7012-7dd4-4169-8630-2a096d587b06","trusted":true},"cell_type":"code","source":"def print_dx_perc(data_frame, col):\n    \"\"\"Function used to print class distribution for our data set\"\"\"\n    try:\n        # Stores value counts\n        col_vals = data_frame[col].value_counts()\n        # Resets index to make index a column in data frame\n        col_vals = col_vals.reset_index()\n        # If the number of unique instances in column exceeds 20 print warning\n        if len(col_vals['index']) > 20:\n            print('Warning: values in column are more than 20 \\nPlease try a column with lower value counts!')\n        # Else it calculates/prints percentage for each unique value in column\n        else:\n            # Create a function to output the percentage\n            f = lambda x, y: 100 * (x / sum(y))\n            for i in range(0, len(col_vals['index'])):\n                print('{0} accounts for {1:.2f}% of the {2} column'\\\n                      .format(col_vals['index'][i],\n                              f(col_vals[col].iloc[i],\n                                col_vals[col]),\n                              col))\n    # try-except block goes here if it can't find the column in data frame\n    except KeyError as e:\n        print('{0}: Not found'.format(e))\n        print('Please choose the right column name!')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3f1699e390548bf1510fae93550021deca769969","_cell_guid":"153b37b8-6e64-4ebd-9049-4e9725c5614d","trusted":true},"cell_type":"code","source":"print_dx_perc(breast_cancer, 'diagnosis')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a1aaaf6f6f452d201a8bb768503ebfb7b436d4ff","_cell_guid":"7537872a-4fc2-4897-9fd8-d20f249bad31"},"cell_type":"markdown","source":"Fortunately, this data set does not suffer from *class imbalance*. \n\nNext, we will employ a function that gives us standard descriptive statistics for each feature including mean, standard deviation, minimum value, maximum value, and range intervals."},{"metadata":{"_uuid":"2540b1b79ae19ecff3180ce4d2fda647aff100d4","_cell_guid":"b21cee85-1489-4150-9fa1-ec14e661e155","trusted":true},"cell_type":"code","source":"breast_cancer.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e3aba52e0038c7ba043ffe3b979f1ebed0833488","_cell_guid":"2d41dd52-86e1-4671-9674-968f21702ce4"},"cell_type":"markdown","source":"You can see in the maximum row of the chart that our data varies in distribution; this will be important as we consider classification models. Standardization is an important requirement for many classification models that should be handled when implementing pre-processing. Some models (like [neural networks](https://www.datascience.com/resources/webinars/introduction-to-neural-nets-with-the-data-incubator)) can perform poorly if pre-processing isn't considered, so the `describe()` function is a good indicator for standardization. Fortunately, random forest does not require any pre-processing. (For use of categorical data, see [sklearn's Encoding Categorical Data](http://scikit-learn.org/stable/modules/preprocessing.html#encoding-categorical-features) section).\n\n# Creating Training and Test Sets\n\nLet's split the data set into our training and test sets, which will be pseudo-randomly selected to create a 80-20% split. You will use the training set to train the model and perform some optimization. You will use the test set, which will act as unseen data, to assess model performance.\n\nWhen using this method for machine learning, always be wary of utilizing your test set to create models. Data leakage is a common problem that can result in overfitting. More on data leakage can be found in this [Kaggle article](https://www.kaggle.com/wiki/Leakage)\n"},{"metadata":{"_uuid":"79da0b2c4c3a172cd795d43215913caf3414334b","collapsed":true,"_cell_guid":"2543fdda-95ce-4c30-92be-48a9f0854002","trusted":true},"cell_type":"code","source":"feature_space = breast_cancer.iloc[:, breast_cancer.columns != 'diagnosis']\nfeature_class = breast_cancer.iloc[:, breast_cancer.columns == 'diagnosis']\n\n\ntraining_set, test_set, class_set, test_class_set = train_test_split(feature_space,\n                                                                    feature_class,\n                                                                    test_size = 0.20, \n                                                                    random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5f0cf1b06c9d0e7fcbe1a435ac4678a8d8b675d3","_cell_guid":"136567c2-cb1f-45af-b9e9-b045bb07093a"},"cell_type":"markdown","source":"**NOTE**: What I mean when I say that we will \"pseudo-randomly\" select data is that we will use a random seed generator and set it equal to a number of our choosing. This will ensure the results are the same for anyone who uses this generator, and therefore, that they will be able to replicate this project. "},{"metadata":{"_uuid":"cdbb02f94cfd6c7b771bf0ea57600392f3349bc5","collapsed":true,"_cell_guid":"4e58a807-e84b-4c91-bfc8-0cea061b455a","trusted":true},"cell_type":"code","source":"# Cleaning test sets to avoid future warning messages\nclass_set = class_set.values.ravel() \ntest_class_set = test_class_set.values.ravel() ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b1128b1559266ade54b2e116b2f680fba1d342a6","_cell_guid":"168946b4-ee4a-46d7-a443-d1faf24885cb"},"cell_type":"markdown","source":"# Fitting Random Forest\n\nNow, let's create the model, starting with parameter tuning. Here are the parameters we will be tuning in this tutorial: \n\n+ **max_depth**: The maximum splits for all trees in the forest.\nbootstrap: An indicator of whether or not we want to use bootstrap samples when building trees.\n+ **max_features**: The maximum number of features that will be used in node splitting — the main difference I previously mentioned between bagging trees and random forest. Typically, you want a value that is less than p, where p is all features in your data set.\n+ **criterion**: This is the metric used to asses the stopping criteria for the decision trees.\n\nOnce we've instantiated our model, we will go ahead and tune our parameters."},{"metadata":{"_uuid":"441444263ba4fd387d204f862d9f7edf6dfee453","collapsed":true,"_cell_guid":"c67c2e0a-41b4-4eb5-aba9-1a0a8c66e0b9","trusted":true},"cell_type":"code","source":"# Set the random state for reproducibility\nfit_rf = RandomForestClassifier(random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"247a8611543cbd1aeece9127cc9a94bf7c337c47","_cell_guid":"14084b7e-0a98-4785-bea6-bee948b321bd"},"cell_type":"markdown","source":"# Hyperparameters Optimization \n\nUtilizing the `GridSearchCV` functionality, let's create a dictionary with parameters we are looking to optimize to create the best model for our data. Setting the `n_jobs` to 3 tells the grid search to run three jobs in parallel, reducing the time the function will take to compute the best parameters. I included the timer to see how long different jobs took; that led me to ultimately decide to use three parallel jobs.\n\nThis will help set the parameters we will use to tune one final parameter: the number of trees in our forest."},{"metadata":{"_uuid":"46ffdd6a8c2b608462d876dc3dd25ce31987db13","_cell_guid":"00f62fdc-8560-49cc-8904-cf475e9b337c","trusted":true},"cell_type":"code","source":"np.random.seed(42)\nstart = time.time()\n\nparam_dist = {'max_depth': [2, 3, 4],\n              'bootstrap': [True, False],\n              'max_features': ['auto', 'sqrt', 'log2', None],\n              'criterion': ['gini', 'entropy']}\n\ncv_rf = GridSearchCV(fit_rf, cv = 10,\n                     param_grid=param_dist, \n                     n_jobs = 3)\n\ncv_rf.fit(training_set, class_set)\nprint('Best Parameters using grid search: \\n', \n      cv_rf.best_params_)\nend = time.time()\nprint('Time taken in grid search: {0: .2f}'.format(end - start))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"51bc07aec43e094de1f0b508c8baab31aea33c33","_cell_guid":"e1f62b4e-0530-4c27-b293-8e1d7c5df24a"},"cell_type":"markdown","source":"Once we are given the best parameter combination, we set the parameters to our model. \n\nNotice how we didn't utilize the `bootstrap: True` parameter, this will make sense in the following section. "},{"metadata":{"_uuid":"c54d806676fc822c9c136463c7f805a8f9abaab3","_cell_guid":"b61acb5c-6654-405e-a1f7-eb37b7b897d5","trusted":true},"cell_type":"code","source":"# Set best parameters given by grid search \nfit_rf.set_params(criterion = 'gini',\n                  max_features = 'log2', \n                  max_depth = 3)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0ce9bae36b4a6f863e8f3499ff71cb7950158171","_cell_guid":"ad47998d-2291-43c4-b15f-420b44b99846"},"cell_type":"markdown","source":"# Out of Bag Error Rate\n\nAnother useful feature of random forest is the concept of an out-of-bag (OOB) error rate. Because only two-thirds of the data are used to train each tree when building the forest, one-third of unseen data can be used in a way that is advantageous to our accuracy metrics without being as computationally expensive as something like cross validation, for instance.\n\nAs outlined below, when calculating OOB, two parameters have to be changed. Also, by utilizing a `for-loop` across a multitude of forest sizes, we can calculate the OOB error rate and use it to asses how many trees are appropriate for our model!\n\n**NOTE**: When calculating the oob score, setting `bootstrap=True` will produce errors, but is necessary for oob_score calculation as stated on this [example](http://scikit-learn.org/stable/auto_examples/ensemble/plot_ensemble_oob.html)\n\nFor the original analysis I compared *Kth Nearest Neighbor*, *Random Forest*, and *Neural Networks*, so most of the analysis was done to compare across different models."},{"metadata":{"_uuid":"aecab108a1baee97c1ce27749c9c5f4cea484348","collapsed":true,"_cell_guid":"1e706829-740a-4162-bad3-c25901405138","trusted":true},"cell_type":"code","source":"fit_rf.set_params(warm_start=True, \n                  oob_score=True)\n\nmin_estimators = 15\nmax_estimators = 1000\n\nerror_rate = {}\n\nfor i in range(min_estimators, max_estimators + 1):\n    fit_rf.set_params(n_estimators=i)\n    fit_rf.fit(training_set, class_set)\n\n    oob_error = 1 - fit_rf.oob_score_\n    error_rate[i] = oob_error","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d8019b4381f27e37497ef7efecac577023acfea7","collapsed":true,"_cell_guid":"91572a4f-a80d-4079-ae14-52a253f10bb4","trusted":true},"cell_type":"code","source":"# Convert dictionary to a pandas series for easy plotting \noob_series = pd.Series(error_rate)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6151561e2011c7de7564b07e93e7a48e87216567","_cell_guid":"4ffe0fb0-b56b-4c2e-bc39-a4a5cf358fb6","trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(10, 10))\n\nax.set_facecolor('#fafafa')\n\noob_series.plot(kind='line',\n                color = 'red')\nplt.axhline(0.055, \n            color='#875FDB',\n           linestyle='--')\nplt.axhline(0.05, \n            color='#875FDB',\n           linestyle='--')\nplt.xlabel('n_estimators')\nplt.ylabel('OOB Error Rate')\nplt.title('OOB Error Rate Across various Forest sizes \\n(From 15 to 1000 trees)')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"270fea494dbf4c88bdb099b0f1cab9ef9b4a0801","_cell_guid":"0fe2d98d-a6f4-4657-8b2c-974bb861133a"},"cell_type":"markdown","source":"The OOB error rate starts to oscilate at around 400 trees, so I will go ahead and use my judgement to use 400 trees in my forest. Using the `pandas` series object I can easily find the OOB error rate for the estimator as follows:"},{"metadata":{"_uuid":"35ea501dfb0e83358e798a42e877e420e85d3963","_cell_guid":"f14327e9-d5e3-46e9-8b00-4319e05f5dad","trusted":true},"cell_type":"code","source":"print('OOB Error rate for 400 trees is: {0:.5f}'.format(oob_series[400]))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"85745e2e9653bb399e9cf6e137851984feaf872d","_cell_guid":"c5f9f29d-32ef-4c23-a8de-49c48c2be5ef"},"cell_type":"markdown","source":"Utilizing the OOB error rate that was created with the model gives us an unbiased error rate. Since OOB can be calculated with the model estimation, it's helpful when cross validating and/or optimizing hyperparameters prove to be too computationally expensive.\n\nFor the sake of this tutorial, I will go over other traditional methods for optimizing machine learning models, including the training and test error route and cross validation metrics.\n\n# Traditional Training and Test Set Split\n\nIn order for this methodology to work we will set the number of trees calculated using the OOB error rate, and removing the `warm_start` and `oob_score` parameters. Along with including the `bootstrap` parameter. "},{"metadata":{"_uuid":"ce63333aa1ca73863004e08991864df47ef674c1","_cell_guid":"2f97e7ee-9eb4-4fb5-9a7a-f0dfc8c7e885","trusted":true},"cell_type":"code","source":"fit_rf.set_params(n_estimators=400,\n                  bootstrap = True,\n                  warm_start=False, \n                  oob_score=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"319fb4bae585a45b1e64ecf25826855bd3195500","_cell_guid":"6d1d0476-8f13-4e41-837b-528067a1c20f"},"cell_type":"markdown","source":"# Training Algorithm\n\nNext we train the algorithm utilizing the training and target class set we had made earlier. "},{"metadata":{"_uuid":"1ef431d2e5f32595ca1360912d9e177a244da6a6","_cell_guid":"8a5149ef-7af9-4ca0-a697-0e2b71ee8ab5","trusted":true},"cell_type":"code","source":"fit_rf.fit(training_set, class_set)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2d8654e20b27eaf896b92de6020f4d83fab82682","_cell_guid":"cf7910f5-fb0b-4989-a50a-5f4f0482da42"},"cell_type":"markdown","source":"# Variable Importance\n\nOnce we have trained the model, we can assess variable importance. One downside to using ensemble methods with decision trees is that you lose the interpretability a single tree gives. A single tree can outline for us important node splits, as well as variables that were important at each split.\n\n\nFortunately, ensemble methods that rely on CART models use a metric to evaluate the homogeneity of splits. Thus, when creating ensembles, these metrics can be utilized to give insight into the important variables used in the training of the model. Two such metrics are **gini impurity** and **entropy**. Many people favor **gini impurity** because it has a lower computational cost than **entropy**, which requires calculating the logarithmic function. For more information, I recommend reading this [article](https://github.com/rasbt/python-machine-learning-book/blob/master/faq/decision-tree-binary.md).\n\nThe two metrics vary and from reading documentation online, many people favor `gini impurity` due to the computational cost of `entropy` since it requires calculating the logarithmic function. For more discussion I recommend reading this [article](https://github.com/rasbt/python-machine-learning-book/blob/master/faq/decision-tree-binary.md).\n\nHere we define each metric:\n\n$$Gini\\ Impurity = 1 - \\sum_i p_i$$\n\nand\n\n$$Entropy = \\sum_i -p_i * \\log_2 p_i$$\n\nwhere $p_i$ is defined as the proportion of subsamples that belong to a certain target class. \n\nSince we are utilizing the gini impurity, the impurity measure reaches 0 when all target class labels are the same.\n\nLet's access the feature importance of the model and use a helper function to output the importance of our variables in descending order."},{"metadata":{"_uuid":"632d5c8683cc9ee76f9a1f07b61dfae37314c1e3","collapsed":true,"_cell_guid":"ef5879e1-dffe-4058-a5fd-4c4f4b89b3b5","trusted":true},"cell_type":"code","source":"def variable_importance(fit):\n    \"\"\"\n    Purpose\n    ----------\n    Checks if model is fitted CART model then produces variable importance\n    and respective indices in dictionary.\n\n    Parameters\n    ----------\n    * fit:  Fitted model containing the attribute feature_importances_\n\n    Returns\n    ----------\n    Dictionary containing arrays with importance score and index of columns\n    ordered in descending order of importance.\n    \"\"\"\n    try:\n        if not hasattr(fit, 'fit'):\n            return print(\"'{0}' is not an instantiated model from scikit-learn\".format(fit)) \n\n        # Captures whether the model has been trained\n        if not vars(fit)[\"estimators_\"]:\n            return print(\"Model does not appear to be trained.\")\n    except KeyError:\n        print(\"Model entered does not contain 'estimators_' attribute.\")\n\n    importances = fit.feature_importances_\n    indices = np.argsort(importances)[::-1]\n    return {'importance': importances,\n            'index': indices}","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d3f75656b36a216834ed7ef7ba892ae4cb496640","collapsed":true,"_cell_guid":"06027b2f-85b4-49e4-b5ea-747c30baf42d","trusted":true},"cell_type":"code","source":"var_imp_rf = variable_importance(fit_rf)\n\nimportances_rf = var_imp_rf['importance']\n\nindices_rf = var_imp_rf['index']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4b089c00d6b88a85ddc37a82a887d79e5174b5d3","_cell_guid":"2dca9bab-379f-47b2-b127-4ae2174be73c","trusted":true,"collapsed":true},"cell_type":"code","source":"def print_var_importance(importance, indices, name_index):\n    \"\"\"\n    Purpose\n    ----------\n    Prints dependent variable names ordered from largest to smallest\n    based on information gain for CART model.\n    Parameters\n    ----------\n    * importance: Array returned from feature_importances_ for CART\n                models organized by dataframe index\n    * indices: Organized index of dataframe from largest to smallest\n                based on feature_importances_\n    * name_index: Name of columns included in model\n\n    Returns\n    ----------\n    Prints feature importance in descending order\n    \"\"\"\n    print(\"Feature ranking:\")\n\n    for f in range(0, indices.shape[0]):\n        i = f\n        print(\"{0}. The feature '{1}' has a Mean Decrease in Impurity of {2:.5f}\"\n              .format(f + 1,\n                      names_index[indices[i]],\n                      importance[indices[f]]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"76d30fd0f0e41734ba2330840828cf3d77be0ab7"},"cell_type":"code","source":"print_var_importance(importances_rf, indices_rf, names_index)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a05eaef7b6a882abe98ff3dc7dfdb825a051b88a","_cell_guid":"82d1a086-cf1c-4c50-a42f-4acd1e77325a"},"cell_type":"markdown","source":"We can see here that our top 5 variables were `area_worst`, `perimeter_worst`, `concave_points_worst`, `concave_points_mean`, `radius_worst`. \n\nThis gives us great insight for further analyses like feature engineering, although I won't get into that topic during this tutorial. It can also give us insight into the mind of the practitioner by showing what variables played an important part in the predictions generated by the model. In the case of our test data set, knowing this information would help practitioners in the medical field focus on the top variables and their relationship with breast cancer."},{"metadata":{"_uuid":"4119677d1be2310bd3b2626764b329692b119776","collapsed":true,"_cell_guid":"6f2318db-fe8c-427f-827a-67019a45cc8a","trusted":true},"cell_type":"code","source":"def variable_importance_plot(importance, indices, name_index):\n    \"\"\"\n    Purpose\n    ----------\n    Prints bar chart detailing variable importance for CART model\n    NOTE: feature_space list was created because the bar chart\n    was transposed and index would be in incorrect order.\n\n    Parameters\n    ----------\n    * importance: Array returned from feature_importances_ for CART\n                models organized by dataframe index\n    * indices: Organized index of dataframe from largest to smallest\n                based on feature_importances_\n    * name_index: Name of columns included in model\n\n    Returns:\n    ----------\n    Returns variable importance plot in descending order\n    \"\"\"\n    index = np.arange(len(names_index))\n\n    importance_desc = sorted(importance)\n    feature_space = []\n    for i in range(indices.shape[0] - 1, -1, -1):\n        feature_space.append(names_index[indices[i]])\n\n    fig, ax = plt.subplots(figsize=(10, 10))\n\n    ax.set_axis_bgcolor('#fafafa')\n    plt.title('Feature importances for Random Forest Model\\\n    \\nBreast Cancer (Diagnostic)')\n    plt.barh(index,\n             importance_desc,\n             align=\"center\",\n             color = '#875FDB')\n    plt.yticks(index,\n               feature_space)\n\n    plt.ylim(-1, 30)\n    plt.xlim(0, max(importance_desc) + 0.01)\n    plt.xlabel('Mean Decrease in Impurity')\n    plt.ylabel('Feature')\n\n    plt.show()\n    plt.close()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"47ce1e503fee449b8dc4a7d6a4ac27d05b58f467","_cell_guid":"5db173a0-e7ed-4646-8958-8b0808c5d8ed","trusted":true},"cell_type":"code","source":"variable_importance_plot(importances_rf, indices_rf, names_index)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3972d89056752839fe007c8bca456bec9c8fd39b","_cell_guid":"49828b2f-1e96-416f-8081-62a536696071"},"cell_type":"markdown","source":"The visual above helps drive home the point, since you can clearly see the difference in the importance of variables for the ensemble method. Certain cutoff points can be made to reduce the inclusion of features and can help in the accuracy of the model, since we'll be removing what is considered noise within our feature space.\n\n\n# Cross Validation\n\nCross validation is a powerful tool that is used for estimating the predictive power of your model, and it performs better than the conventional training and test set. Using cross validation, we can create multiple training and test sets and average the scores to give us a less biased metric.\n\nIn this case, we will create 10 sets within our data set that calculate the estimations we have done already, then average the prediction error to give us a more accurate representation of our model's prediction power. The model's performance can vary significantly when utilizing different training and test sets.\n\n**Suggested Reading**: For a more concise explanation of *Cross Validation* I recommend reading [An Introduction to Statistical Learnings with Applications in R](http://www-bcf.usc.edu/~gareth/ISL/), specifically chapter 5.1!\n\n## K-Fold Cross Validation\n\nHere we are employing K-fold cross validation; more specifically, 10 folds. We are creating 10 subsets of our data on which to employ the training and test set methodology; then we will average the accuracy for all folds to give us our estimation.\n\nWithin a random forest context, if your data set is significantly large, you can choose to not do cross validation and instead use the OOB error rate as an unbiased metric for computational costs. But for the purposes of this tutorial, I included it to show the different accuracy metrics available."},{"metadata":{"_uuid":"cc361804b0a1c02d7750a0817da9449e43daecab","collapsed":true,"_cell_guid":"4aeddfa8-03ba-4058-a30c-c9db00f675b9","trusted":true},"cell_type":"code","source":"def cross_val_metrics(fit, training_set, class_set, estimator, print_results = True):\n    \"\"\"\n    Purpose\n    ----------\n    Function helps automate cross validation processes while including \n    option to print metrics or store in variable\n\n    Parameters\n    ----------\n    fit: Fitted model \n    training_set:  Data_frame containing 80% of original dataframe\n    class_set:     data_frame containing the respective target vaues \n                      for the training_set\n    print_results: Boolean, if true prints the metrics, else saves metrics as \n                      variables\n\n    Returns\n    ----------\n    scores.mean(): Float representing cross validation score\n    scores.std() / 2: Float representing the standard error (derived\n                from cross validation score's standard deviation)\n    \"\"\"\n    my_estimators = {\n    'rf': 'estimators_',\n    'nn': 'out_activation_',\n    'knn': '_fit_method'\n    }\n    try:\n        # Captures whether first parameter is a model\n        if not hasattr(fit, 'fit'):\n            return print(\"'{0}' is not an instantiated model from scikit-learn\".format(fit)) \n\n        # Captures whether the model has been trained\n        if not vars(fit)[my_estimators[estimator]]:\n            return print(\"Model does not appear to be trained.\")\n\n    except KeyError as e:\n        print(\"'{0}' does not correspond with the appropriate key inside the estimators dictionary. \\\n\\nPlease refer to function to check `my_estimators` dictionary.\".format(estimator))\n        raise\n\n    n = KFold(n_splits=10)\n    scores = cross_val_score(fit, \n                         training_set, \n                         class_set, \n                         cv = n)\n    if print_results:\n        for i in range(0, len(scores)):\n            print(\"Cross validation run {0}: {1: 0.3f}\".format(i, scores[i]))\n        print(\"Accuracy: {0: 0.3f} (+/- {1: 0.3f})\"\\\n              .format(scores.mean(), scores.std() / 2))\n    else:\n        return scores.mean(), scores.std() / 2","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a151b9a949012caf522a41b9cc4164d2f40b03ef","_cell_guid":"f786e030-7c38-4ba7-8a04-3e87870b0c79","trusted":true},"cell_type":"code","source":"cross_val_metrics(fit_rf, \n                  training_set, \n                  class_set, \n                  'rf',\n                  print_results = True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f40125884ba3c5290e5514ec93af641d83595202","_cell_guid":"6ad41664-63bc-4a85-970f-24bc79ccaacc"},"cell_type":"markdown","source":"# Test Set Metrics\n\nUsing the test set that was created earlier, let's examine another metric for the evaluation of our model. You'll recall that that we didn't touch the test set until now — after we had completed hyperparamter optimization — to avoid the problem of data leakage. \n\nHere, I have created a confusion matrix showcasing the following metrics:\n\n| n = Sample Size | Predicted Benign | Predicted Malignant | \n|-----------------|------------------|---------------------|\n| Actual Benign | *True Negative* | *False Positive* | \n| Actual Malignant | *False Negative* | *True Positive* | "},{"metadata":{"_uuid":"a3fee0fe661cdb2af3544e108db709f2954612df","collapsed":true,"_cell_guid":"3404ede1-0a0c-4ba4-9b75-d217ea966053","trusted":true},"cell_type":"code","source":"predictions_rf = fit_rf.predict(test_set)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7dee642aea7d223c7d7cb82765dd3e6ee1cfcd4f","_cell_guid":"d44bf4e9-932d-4c5a-8edc-33e36109e6de","trusted":false,"collapsed":true},"cell_type":"markdown","source":"## Confusion Matrix\nHere we create a confusion matrix visual with `seaborn` and transposing the matrix when creating the heatmap. "},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"7a7776641814aab626bee22bf25dff6e43e9463a"},"cell_type":"code","source":"def create_conf_mat(test_class_set, predictions):\n    \"\"\"Function returns confusion matrix comparing two arrays\"\"\"\n    if (len(test_class_set.shape) != len(predictions.shape) == 1):\n        return print('Arrays entered are not 1-D.\\nPlease enter the correctly sized sets.')\n    elif (test_class_set.shape != predictions.shape):\n        return print('Number of values inside the Arrays are not equal to each other.\\nPlease make sure the array has the same number of instances.')\n    else:\n        # Set Metrics\n        test_crosstb_comp = pd.crosstab(index = test_class_set,\n                                        columns = predictions)\n        # Changed for Future deprecation of as_matrix\n        test_crosstb = test_crosstb_comp.values\n        return test_crosstb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f830a8526afb227ee195d69d54b4f471ad411c0d"},"cell_type":"code","source":"conf_mat = create_conf_mat(test_class_set, predictions_rf)\nsns.heatmap(conf_mat, annot=True, fmt='d', cbar=False)\nplt.xlabel('Predicted Values')\nplt.ylabel('Actual Values')\nplt.title('Actual vs. Predicted Confusion Matrix')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"11cb60440443a5ba9dadd77e778141d3cf37bcff","_cell_guid":"9d32333b-7e70-4755-94e4-2e4a2048af87","trusted":true},"cell_type":"code","source":"accuracy_rf = fit_rf.score(test_set, test_class_set)\n\nprint(\"Here is our mean accuracy on the test set:\\n {0:.3f}\"\\\n      .format(accuracy_rf))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bb20c31abec3597a258586848fe77a48e1472e88","_cell_guid":"da61f588-d1be-464a-b5e6-bd8ae997c99a","trusted":true},"cell_type":"code","source":"# Here we calculate the test error rate!\ntest_error_rate_rf = 1 - accuracy_rf\nprint(\"The test error rate for our model is:\\n {0: .4f}\"\\\n      .format(test_error_rate_rf))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c518ef0ffdc49ede8d91f25e7af48c1c3df896dd","_cell_guid":"677f98c5-4683-4200-bf2d-90a622828d36"},"cell_type":"markdown","source":"As you can see we got a very similar error rate for our test set that we did for our OOB, which is a good sign for our model. \n\n# ROC Curve Metrics\n\nA receiver operating characteristic (ROC) curve calculates the false positive rates and true positive rates across different thresholds. Let's graph these calculations.\n\nIf our curve is located in the top left corner of the plot, that indicates an ideal model; i.e., a false positive rate of 0 and true positive rate of 1. On the other hand, a ROC curve that is at 45 degrees is indicative of a model that is essentially randomly guessing.\n\nWe will also calculate the area under the curve (AUC). The AUC is used as a metric to differentiate the prediction power of the model for patients with cancer and those without it. Typically, a value closer to 1 means that our model was able to differentiate correctly from a random sample of the two target classes of two patients with and without the disease. "},{"metadata":{"_uuid":"193c386ebe03ddae2f8d6424db505a06127b0524","collapsed":true,"_cell_guid":"756c60a3-547d-4663-9508-accfe720c3cf","trusted":true},"cell_type":"code","source":"# We grab the second array from the output which corresponds to\n# to the predicted probabilites of positive classes \n# Ordered wrt fit.classes_ in our case [0, 1] where 1 is our positive class\npredictions_prob = fit_rf.predict_proba(test_set)[:, 1]\n\nfpr2, tpr2, _ = roc_curve(test_class_set,\n                          predictions_prob,\n                          pos_label = 1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c1881da0a8e04796324114ec5e2763c7d84cce38","collapsed":true,"_cell_guid":"b5f68f91-80ae-4649-8b5d-1e6587acbf5a","trusted":true},"cell_type":"code","source":"auc_rf = auc(fpr2, tpr2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e2893195dd32c987d103f8c73a85ee32551728f5","collapsed":true,"_cell_guid":"82d5629a-056a-4db7-b578-3aad90b8e1fb","trusted":true},"cell_type":"code","source":"def plot_roc_curve(fpr, tpr, auc, estimator, xlim=None, ylim=None):\n    \"\"\"\n    Purpose\n    ----------\n    Function creates ROC Curve for respective model given selected parameters.\n    Optional x and y limits to zoom into graph\n\n    Parameters\n    ----------\n    * fpr: Array returned from sklearn.metrics.roc_curve for increasing\n            false positive rates\n    * tpr: Array returned from sklearn.metrics.roc_curve for increasing\n            true positive rates\n    * auc: Float returned from sklearn.metrics.auc (Area under Curve)\n    * estimator: String represenation of appropriate model, can only contain the\n    following: ['knn', 'rf', 'nn']\n    * xlim: Set upper and lower x-limits\n    * ylim: Set upper and lower y-limits\n    \"\"\"\n    my_estimators = {'knn': ['Kth Nearest Neighbor', 'deeppink'],\n              'rf': ['Random Forest', 'red'],\n              'nn': ['Neural Network', 'purple']}\n\n    try:\n        plot_title = my_estimators[estimator][0]\n        color_value = my_estimators[estimator][1]\n    except KeyError as e:\n        print(\"'{0}' does not correspond with the appropriate key inside the estimators dictionary. \\\n\\nPlease refer to function to check `my_estimators` dictionary.\".format(estimator))\n        raise\n\n    fig, ax = plt.subplots(figsize=(10, 10))\n    ax.set_facecolor('#fafafa')\n\n    plt.plot(fpr, tpr,\n             color=color_value,\n             linewidth=1)\n    plt.title('ROC Curve For {0} (AUC = {1: 0.3f})'\\\n              .format(plot_title, auc))\n\n    plt.plot([0, 1], [0, 1], 'k--', lw=2) # Add Diagonal line\n    plt.plot([0, 0], [1, 0], 'k--', lw=2, color = 'black')\n    plt.plot([1, 0], [1, 1], 'k--', lw=2, color = 'black')\n    if xlim is not None:\n        plt.xlim(*xlim)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.show()\n    plt.close()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3cd137e1f120448f8a64bca70ef87314e9ea4b9b","_cell_guid":"f036dc38-ef7d-40b7-b771-7b93fd969c51","trusted":true},"cell_type":"code","source":"plot_roc_curve(fpr2, tpr2, auc_rf, 'rf',\n               xlim=(-0.01, 1.05), \n               ylim=(0.001, 1.05))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2279469a23d5e1de8399ef1f5841ccbfd33225d6","_cell_guid":"37837d67-7a32-4f1d-8c68-2cc6a807e84c"},"cell_type":"markdown","source":"Our model did exceptional with an AUC over .90, now we do a zoomed in view to showcase the closeness our ROC Curve is relative to the ideal ROC Curve. "},{"metadata":{"_uuid":"7d08f886cad59d275f2a7ff64ad19c1294779208","_cell_guid":"37a72871-a072-466f-8047-4387e3502127","trusted":true},"cell_type":"code","source":"plot_roc_curve(fpr2, tpr2, auc_rf, 'rf', \n               xlim=(-0.01, 0.2), \n               ylim=(0.85, 1.01))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9b22421b97c57962d461a819cb92a2ac39a3920c","_cell_guid":"01758204-a3cb-4a38-bf7b-60a96e5ad9db"},"cell_type":"markdown","source":"# Classification Report\n\nThe classification report is available through `sklearn.metrics`, this report gives many important classification metrics including:\n+ `Precision`: also the positive predictive value, is the number of correct predictions divided by the number of correct predictions plus false positives, so $tp / (tp + fp)$\n+ `Recall`: also known as the sensitivity, is the number of correct predictions divided by the total number of instances so $tp / (tp + fn)$ where $fn$ is the number of false negatives\n+ `f1-score`: this is defined as the *weighted harmonic mean* of both the precision and recall, where the f1-score at 1 is the best value and worst value at 0, as defined by the [documentation](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html#sklearn.metrics.precision_recall_fscore_support)\n+ `support`: number of instances that are the correct target values\n\nAcross the board, we can see that our model provided great insight into classifying patients based on FNA scans. Other important metrics to consider would be the false positive rate, since within this context it would be bad for the model to tell someone that they are cancer free when in reality they have cancer.\n"},{"metadata":{"_uuid":"37c0fc4c2b76e2f4f15a7df7bed169ae2a5de6db","collapsed":true,"_cell_guid":"80b3ff3f-ffce-4ab0-bec2-eab7edc5e8a5","trusted":true},"cell_type":"code","source":"def print_class_report(predictions, alg_name):\n    \"\"\"\n    Purpose\n    ----------\n    Function helps automate the report generated by the\n    sklearn package. Useful for multiple model comparison\n\n    Parameters:\n    ----------\n    predictions: The predictions made by the algorithm used\n    alg_name: String containing the name of the algorithm used\n    \n    Returns:\n    ----------\n    Returns classification report generated from sklearn. \n    \"\"\"\n    print('Classification Report for {0}:'.format(alg_name))\n    print(classification_report(predictions, \n            test_class_set, \n            target_names = dx))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3c70cd76e66f66ea87eb90f754c5f322637726a5","_cell_guid":"b206ef9d-0cd8-4d3b-86b5-a7570b72c390","trusted":true},"cell_type":"code","source":"class_report = print_class_report(predictions_rf, 'Random Forest')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"94e030136aa9c5bb30dd8839b3be000a29021f20","_cell_guid":"0eb9d0df-e915-4475-849d-665eae6d766d"},"cell_type":"markdown","source":"## Metrics for Random Forest\n\nHere I've accumulated the various metrics we used through this tutorial in a simple table! Showcasing the power and effectiveness of Random Forest Modeling. \n\n| Model | OOB Error Rate | Test Error Rate | Cross Validation Score | AUC | \n|-------|----------------|------------------------|-----------------|-----|\n| Random Forest | 0.04835 |  0.0351 | 0.947 (+/-  0.019) | 0.967 | \n\n# Conclusions\n\nWe've now gone through a number of metrics to assess the capabilities of our random forest, but there's still much that can be done using background information from the data set. Feature engineering would be a powerful tool for extracting information and moving forward into the research phase, and would help define key metrics to utilize when optimizing model parameters.\n\nThere have been advancements with image classification in the past decade that make it possible to use images instead of extracted features from those images, but this data set is a great resource for making use of machine learning processes and concepts. If you have any suggestions, recommendations, or corrections please reach out to me.\n\n"},{"metadata":{"_uuid":"da898cfb1ab1c3a3e6f4ce7ae26b8b83df80e18d","collapsed":true,"_cell_guid":"8c4b056d-6451-46c1-b6f5-0685257f34c4","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}